{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7 Saving and Loading Models.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNyBes5jDoVdRuUN0y7wdxU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dD2405/Intro_to_TensorFlow/blob/master/7_Saving_and_Loading_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l6TUpiCd8PN",
        "colab_type": "text"
      },
      "source": [
        "# Saving and Loading Models\n",
        "\n",
        "In this tutorial we will learn how we can take a trained model, save it, and then load it back to keep training it or use it to perform inference. In particular, we will use transfer learning to train a classifier to classify images of cats and dogs, just like we did in the previous lesson. We will then take our trained model and save it as an HDF5 file, which is the format used by Keras. We will then load this model, use it to perform predictions, and then continue to train the model. Finally, we will save our trained model as a TensorFlow SavedModel and then we will download it to a local disk, so that it can later be used for deployment in different platforms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCP_s2DPd9OB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Concepts that will be covered in this Colab\n",
        "\n",
        "1. Saving models in HDF5 format for Keras\n",
        "2. Saving models in the TensorFlow SavedModel format\n",
        "3. Loading models\n",
        "4. Download models to Local Disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWLwyoyrd_sR",
        "colab_type": "text"
      },
      "source": [
        "# Imports\n",
        "\n",
        "In this Colab we will use the TensorFlow 2.0 Beta version. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOx0_VLQbp6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f68e3de-7f00-4b68-b9ac-dd506cab464c"
      },
      "source": [
        "try:\n",
        "  # Use the %tensorflow_version magic if in colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  !pip install -U \"tensorflow-gpu==2.0.0rc0\" "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FwtQ5rRbtUm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "a0da532c-782b-44f0-c2df-7954b2748cfc"
      },
      "source": [
        "!pip install -U tensorflow_hub\n",
        "!pip install -U tensorflow_datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow_hub in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /tensorflow-2.1.0/python3.6 (from tensorflow_hub) (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /tensorflow-2.1.0/python3.6 (from tensorflow_hub) (3.11.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /tensorflow-2.1.0/python3.6 (from tensorflow_hub) (1.18.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /tensorflow-2.1.0/python3.6 (from protobuf>=3.4.0->tensorflow_hub) (45.0.0)\n",
            "Collecting tensorflow_datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b9/74c219b0310b3df0ac60c4948c4191b9377b6b746615b176819533096fb5/tensorflow_datasets-2.0.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (19.3.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (1.18.1)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (3.11.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (2.22.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /tensorflow-2.1.0/python3.6 (from protobuf>=3.6.1->tensorflow_datasets) (45.0.0)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests>=2.19.0->tensorflow_datasets) (1.25.7)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests>=2.19.0->tensorflow_datasets) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests>=2.19.0->tensorflow_datasets) (2019.11.28)\n",
            "Installing collected packages: tensorflow-datasets\n",
            "  Found existing installation: tensorflow-datasets 1.3.2\n",
            "    Uninstalling tensorflow-datasets-1.3.2:\n",
            "      Successfully uninstalled tensorflow-datasets-1.3.2\n",
            "Successfully installed tensorflow-datasets-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRrCt9OTbuJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZII1mEbbv3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KKRPuOkeHWc",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Load the Cats vs. Dogs Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSHrkYShbxgB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "48ea3532-62dd-40a3-852b-20c713617edc"
      },
      "source": [
        "(train_examples, validation_examples), info = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    split=['train[:80%]', 'train[80%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset cats_vs_dogs (786.68 MiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "WARNING:absl:1738 images were corrupted and were skipped\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shuffling and writing examples to /root/tensorflow_datasets/cats_vs_dogs/4.0.0.incomplete0DHAUO/cats_vs_dogs-train.tfrecord\n",
            "\u001b[1mDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_al5tuMeKk5",
        "colab_type": "text"
      },
      "source": [
        "The images in the Dogs vs. Cats dataset are not all the same size. So, we need to reformat all images to the resolution expected by MobileNet (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ6JfuOzb0Xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_image(image, label):\n",
        "  # `hub` image modules exepct their data normalized to the [0,1] range.\n",
        "  image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES))/255.0\n",
        "  return  image, label\n",
        "\n",
        "num_examples = info.splits['train'].num_examples\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_RES = 224\n",
        "\n",
        "train_batches      = train_examples.cache().shuffle(num_examples//4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "validation_batches = validation_examples.cache().map(format_image).batch(BATCH_SIZE).prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLu3ZtrfeSeg",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Transfer Learning with TensorFlow Hub\n",
        "\n",
        "We will now use TensorFlow Hub to do Transfer Learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAfIPD9Rb2F4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
        "feature_extractor = hub.KerasLayer(URL,\n",
        "                                   input_shape=(IMAGE_RES, IMAGE_RES,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bbAKVVGb2Nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_extractor.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfmdhOVeVXJ",
        "colab_type": "text"
      },
      "source": [
        "## Attach a classification head\n",
        "\n",
        "Now wrap the hub layer in a `tf.keras.Sequential` model, and add a new classification layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvaCqCy6b2XI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "7278509a-7b75-453f-be72-8baa75aee111"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_extractor,\n",
        "  layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 1280)              2257984   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 2562      \n",
            "=================================================================\n",
            "Total params: 2,260,546\n",
            "Trainable params: 2,562\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvXrQFBEeYVB",
        "colab_type": "text"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "We now train this model like any other, by first calling `compile` followed by `fit`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3uckjyXb7ug",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "30852bc1-a5ef-4d0d-a41f-1a80eb1233f6"
      },
      "source": [
        "model.compile(\n",
        "  optimizer='adam', \n",
        "  loss=tf.losses.SparseCategoricalCrossentropy(),\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "EPOCHS = 3\n",
        "history = model.fit(train_batches,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=validation_batches)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "582/582 [==============================] - 29s 50ms/step - loss: 0.0500 - accuracy: 0.9834 - val_loss: 0.0301 - val_accuracy: 0.9897\n",
            "Epoch 2/3\n",
            "582/582 [==============================] - 17s 29ms/step - loss: 0.0280 - accuracy: 0.9904 - val_loss: 0.0303 - val_accuracy: 0.9895\n",
            "Epoch 3/3\n",
            "582/582 [==============================] - 17s 29ms/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 0.0294 - val_accuracy: 0.9912\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}