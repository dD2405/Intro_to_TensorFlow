{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifying images of clothing(MNIST).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dD2405/Intro_to_TensorFlow/blob/master/Classifying_images_of_clothing(MNIST).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z73l28vtvw8",
        "colab_type": "text"
      },
      "source": [
        "## Install and import dependencies\n",
        "\n",
        "We'll need [TensorFlow Datasets](https://www.tensorflow.org/datasets/), an API that simplifies downloading and accessing datasets, and provides several sample datasets to work with. We're also using a few helper libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePwdtksm79CY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "52898e31-7239-46df-ae22-d9afcdbb3160"
      },
      "source": [
        "!pip install -U tensorflow_datasets"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied, skipping upgrade: attrs in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (19.3.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.2.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (1.13.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (2.22.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (3.11.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor in /tensorflow-2.1.0/python3.6 (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests>=2.19.0->tensorflow_datasets) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests>=2.19.0->tensorflow_datasets) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests>=2.19.0->tensorflow_datasets) (1.25.7)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /tensorflow-2.1.0/python3.6 (from protobuf>=3.6.1->tensorflow_datasets) (42.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7faMrM4AuGMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division , print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsO0XymCvOHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdnR5cuUuf2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import tensorflow datasets\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "#helper libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zBKWTmhvFmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4oU9PHwM_c",
        "colab_type": "text"
      },
      "source": [
        "## Import the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuh49xm8wH8k",
        "colab_type": "text"
      },
      "source": [
        "This guide uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 $\\times$ 28 pixels), as seen here:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\" width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset—often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we'll use here.\n",
        "\n",
        "This guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n",
        "\n",
        "We will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow, using the [Datasets](https://www.tensorflow.org/datasets) API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzf8QPvrwJZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset , metadata = tfds.load('fashion_mnist' , as_supervised=True , with_info=True)\n",
        "\n",
        "train_dataset, test_dataset = dataset['train'] , dataset['test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCyR5G-t1kqb",
        "colab_type": "text"
      },
      "source": [
        "Loading the dataset returns metadata as well as a *training dataset* and *test dataset*.\n",
        "\n",
        "* The model is trained using `train_dataset`.\n",
        "* The model is tested against `test_dataset`.\n",
        "\n",
        "The images are 28 $\\times$ 28 arrays, with pixel values in the range `[0, 255]`. The *labels* are an array of integers, in the range `[0, 9]`. These correspond to the *class* of clothing the image represents:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3awGUH0R1vcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02fa4676-0bf3-427d-e301-2b72b6580567"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_OptionsDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioRb1yEz2VID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5219011-8908-4c94-db3c-ab864f0d9490"
      },
      "source": [
        "test_dataset"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_OptionsDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOYn-7wN0P81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9CQV_Bw2nNF",
        "colab_type": "text"
      },
      "source": [
        "### Explore the data\n",
        "\n",
        "Let's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, and 10000 images in the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d745AcG1ozh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "349aea4e-366c-4bbe-9f9a-222e0a281e7c"
      },
      "source": [
        "num_train_examples = metadata.splits['train'].num_examples\n",
        "num_test_examples = metadata.splits['test'].num_examples\n",
        "\n",
        "print('Number of training examples:{}'.format(num_train_examples))\n",
        "print('Number of testing examples:{}'.format(num_test_examples))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples:60000\n",
            "Number of testing examples:10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pMcIN695um7",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "The value of each pixel in the image data is an integer in the range `[0,255]`. For the model to work properly, these values need to be normalized to the range `[0,1]`. So here we create a normalization function, and then apply it to each image in the test and train datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7I0hBv-5gp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(images,labels):\n",
        "    images = tf.cast(images , tf.float32)\n",
        "    images /= 255\n",
        "    return images , labels\n",
        "\n",
        "#The map function applies the normalize() function to each element in the \n",
        "# train and test datasets\n",
        "train_dataset = train_dataset.map(normalize)\n",
        "test_dataset = test_dataset.map(normalize)\n",
        "\n",
        "# The first time you use the dataset, the images will be loaded from disk\n",
        "# Caching will keep them in memory, making training faster\n",
        "train_dataset = train_dataset.cache()\n",
        "test_dataset = test_dataset.cache()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SahROg-8c2V",
        "colab_type": "text"
      },
      "source": [
        "### Explore the processed data\n",
        "\n",
        "Let's plot an image to see what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUCHH0Tv7ta_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "f2b3e374-505c-46ac-980a-8f2fbe287677"
      },
      "source": [
        "for image,labels in test_dataset.take(1):\n",
        "    break\n",
        "\n",
        "image = image.numpy().reshape((28,28))\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(image,cmap=plt.cm.binary)\n",
        "plt.colorbar\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z78Uhy8ZGVHc",
        "colab_type": "text"
      },
      "source": [
        "Display the first 25 images from the *training set* and display the class name below each image. Verify that the data is in the correct format and we're ready to build and train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm5JQ6iWCRos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "i = 0\n",
        "\n",
        "for image,labels in test_dataset.take(25):\n",
        "    image = image.numpy().reshape((28,28))\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(image, plt.cm.binary)\n",
        "    plt.xlabel(class_names[labels])\n",
        "    i += 1\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoFxdmyHGemr",
        "colab_type": "text"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Building the neural network requires configuring the layers of the model, then compiling the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sPdvyG2Gewq",
        "colab_type": "text"
      },
      "source": [
        "### Setup the layers\n",
        "\n",
        "The basic building block of a neural network is the *layer*. A layer extracts a representation from the data fed into it. Hopefully, a series of connected layers results in a representation that is meaningful for the problem at hand.\n",
        "\n",
        "Much of deep learning consists of chaining together simple layers. Most layers, like `tf.keras.layers.Dense`, have internal parameters which are adjusted (\"learned\") during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfYJLc5DF23_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                             tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                             tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "                             ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azYDeJxCIKpd",
        "colab_type": "text"
      },
      "source": [
        "This network has three layers:\n",
        "\n",
        "* **input** `tf.keras.layers.Flatten` — This layer transforms the images from a 2d-array of 28 $\\times$ 28 pixels, to a 1d-array of 784 pixels (28\\*28). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn, as it only reformats the data.\n",
        "\n",
        "* **\"hidden\"** `tf.keras.layers.Dense`— A densely connected layer of 128 neurons. Each neuron (or node) takes input from all 784 nodes in the previous layer, weighting that input according to hidden parameters which will be learned during training, and outputs a single value to the next layer.\n",
        "\n",
        "* **output** `tf.keras.layers.Dense` — A 10-node *softmax* layer, with each node representing a class of clothing. As in the previous layer, each node takes input from the 128 nodes in the layer before it. Each node weights the input according to learned parameters, and then outputs a value in the range `[0, 1]`, representing the probability that the image belongs to that class. The sum of all 10 node values is 1.\n",
        "\n",
        "\n",
        "### Compile the model\n",
        "\n",
        "Before the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n",
        "\n",
        "\n",
        "* *Loss function* — An algorithm for measuring how far the model's outputs are from the desired output. The goal of training is this measures loss.\n",
        "* *Optimizer* —An algorithm for adjusting the inner parameters of the model in order to minimize loss.\n",
        "* *Metrics* —Used to monitor the training and testing steps. The following example uses *accuracy*, the fraction of the images that are correctly classified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebEdxAvSIBuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz6xOhXVKrnC",
        "colab_type": "text"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "First, we define the iteration behavior for the train dataset:\n",
        "1. Repeat forever by specifying `dataset.repeat()` (the `epochs` parameter described below limits how long we perform training).\n",
        "2. The `dataset.shuffle(60000)` randomizes the order so our model cannot learn anything from the order of the examples.\n",
        "3. And `dataset.batch(32)` tells `model.fit` to use batches of 32 images and labels when updating the model variables.\n",
        "\n",
        "Training is performed by calling the `model.fit` method:\n",
        "1. Feed the training data to the model using `train_dataset`.\n",
        "2. The model learns to associate images and labels.\n",
        "3. The `epochs=5` parameter limits training to 5 full iterations of the training dataset, so a total of 5 * 60000 = 300000 examples.\n",
        "\n",
        "(Don't worry about `steps_per_epoch`, the requirement to have this flag will soon be removed.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZJd_YJBKhBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataset = train_dataset.repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx3BYykuLiPB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "bc6c66bd-042e-49d4-b648-5881417ffebd"
      },
      "source": [
        "model.fit(train_dataset, epochs=5, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-8ed533787d52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_examples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, standardize_function, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstandardize_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;31m# Note that the dataset instance is immutable, its fine to reusing the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mstandardize_function\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    682\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m       return ParallelMapDataset(\n\u001b[0;32m-> 1592\u001b[0;31m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3925\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3926\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3927\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3928\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n\u001b[1;32m   3929\u001b[0m         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3146\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3147\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3148\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 2395\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2396\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3139\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3140\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3141\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3142\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3081\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3083\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3084\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py:677 map_fn\n        batch_size=None)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py:2410 _standardize_tensors\n        exception_prefix='input')\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_utils.py:573 standardize_input_data\n        'with shape ' + str(data_shape))\n\n    ValueError: Error when checking input: expected flatten_1_input to have 4 dimensions, but got array with shape (None, None, None, None, None, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9phGwEwZLwnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}